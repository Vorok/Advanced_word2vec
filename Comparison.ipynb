{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "class WordEmbeddings():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.word_frequency = dict()\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.embeddings = None\n",
    "        for i, line in enumerate(open(self.filename, encoding=\"utf8\")):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.vocab_size = int(line[0])\n",
    "                self.embedding_size = int(line[1])\n",
    "                self.embeddings = np.zeros((self.vocab_size, self.embedding_size))\n",
    "                continue\n",
    "            line = line.split(' ', 1)\n",
    "            word = line[0]\n",
    "            self.word2id[word] = i - 1\n",
    "            self.id2word[i - 1] = word\n",
    "            self.embeddings[i - 1, :] = np.fromstring(line[1], dtype=float, sep=' ')\n",
    "    \n",
    "    def embedding_for(self, word):\n",
    "        ind = self.word2id[word]\n",
    "        return self.embeddings[ind, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_penn = WordEmbeddings(\"./embeddings/out_enwik8_penn_500dim_5wind.vec\")\n",
    "we_word2vec = WordEmbeddings(\"./embeddings/out_enwik8_w2v.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def vectorize_word(we, word):\n",
    "    vec = None\n",
    "    if isinstance(word, np.ndarray):\n",
    "        vec = word\n",
    "    if isinstance(word, str):\n",
    "        vec = we.embedding_for(word)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    vec = vec.reshape((1, -1))\n",
    "    return cosine_similarity(vec, vec2)\n",
    "  \n",
    "    \n",
    "def nearest_words(we, word, top_n=10):\n",
    "    vec = vectorize_word(we, word)\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def nearest_words_to_pairs_addition(we, word_a, word_b, top_n=10):\n",
    "    vec_a = vectorize_word(we, word_a)\n",
    "    vec_b = vectorize_word(we, word_b)\n",
    "    vec = vec_a + vec_b\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def best_cosine(we, top_n=10):\n",
    "    best_cosins = np.zeros(we.vocab_size, dtype=\"float\")\n",
    "    best_cos_pair = np.zeros(we.vocab_size, dtype=\"int64\")\n",
    "    for ind in tqdm(we.word2id.values()):\n",
    "        #ind = 0\n",
    "        vec = we.embeddings[ind, :]\n",
    "        vec = vec.reshape((1, -1))\n",
    "        cosines = cosine_similarity(vec, we.embeddings)\n",
    "        word_id = np.argsort(cosines)[0][::-1][1] \n",
    "        best_cosins[ind] = cosines[0][word_id]\n",
    "        best_cos_pair[ind] = word_id\n",
    "    top_cos_args_id = np.argmax(best_cosins)[:top_n+1]\n",
    "    best_pairs = [(we.id2word[ind], we.id2word[best_cos_pair[ind]], best_cosins[ind]) \n",
    "                  for i, ind in enumerate(top_cos_args_id)]\n",
    "    return best_pairs\n",
    "    \n",
    "    \n",
    "def compare(we_a, we_b, word_a, word_b):\n",
    "    print(\"Words: {} + {}\".format(word_a, word_b))\n",
    "    print(\"Word2vec addition:\\n\", nearest_words_to_pairs_addition(we_a, word_b, word_a), \"\\n\")\n",
    "    print(\"PENN addition:\\n\", nearest_words_to_pairs_addition(we_b, word_b, word_a), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████▎             | 9641/11731 [10:40<02:27, 14.17it/s]"
     ]
    }
   ],
   "source": [
    "best_cosine(we_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('capitalist', 0.9329881297461052),\n",
       " ('capitalism', 0.9026125478341888),\n",
       " ('anarchist', 0.8945130504426946),\n",
       " ('anarcho', 0.8944931630226096),\n",
       " ('libertarian', 0.8798084819004081),\n",
       " ('faire', 0.867901212119998),\n",
       " ('liberalism', 0.8544044630409507),\n",
       " ('laissez', 0.8279008774309387),\n",
       " ('rothbard', 0.8255568321545554),\n",
       " ('communism', 0.8160838067515472)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_word2vec, \"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('individualist', 0.8593052069099795),\n",
       " ('anarchist', 0.8471530492275303),\n",
       " ('rothbard', 0.8316277232633484),\n",
       " ('metaphysical', 0.8207106134074114),\n",
       " ('zionism', 0.8126567946987308),\n",
       " ('rejection', 0.8126141317278897),\n",
       " ('authoritarian', 0.8109840579306388),\n",
       " ('contend', 0.8101501365925213),\n",
       " ('aclu', 0.8078392413248282),\n",
       " ('contradict', 0.8075189621300534)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_penn, \"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soviet', 0.8919573507156756),\n",
       " ('guerrilla', 0.7668216671653967),\n",
       " ('warsaw', 0.7589129466196749),\n",
       " ('dissident', 0.7412740590799167),\n",
       " ('veteran', 0.7346475360194393),\n",
       " ('coup', 0.7303312014641579),\n",
       " ('liberate', 0.7273080260705842),\n",
       " ('neutrality', 0.7189481155538215),\n",
       " ('pact', 0.7172397340185512),\n",
       " ('ussr', 0.712835335414549)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_word2vec, 'soviet', 'union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('union', 0.8449629208665597),\n",
       " ('confederate', 0.6656608921533554),\n",
       " ('slave', 0.6369609214171088),\n",
       " ('invasion', 0.6344407750103969),\n",
       " ('warsaw', 0.6328482311938771),\n",
       " ('ally', 0.6253420991806693),\n",
       " ('secession', 0.6177441658627295),\n",
       " ('occupation', 0.6123611501317596),\n",
       " ('seize', 0.598720415174633),\n",
       " ('liberty', 0.5952388067064349)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_penn, 'soviet', 'union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: boy + girl\n",
      "Word2vec addition:\n",
      " [('boy', 0.90739676267495), ('thirteen', 0.729378889200718), ('teenage', 0.7238851521057705), ('beautiful', 0.7080131873160405), ('rap', 0.7046934852354403), ('astro', 0.6931712679654263), ('marple', 0.69063418681593), ('sibling', 0.685468327135792), ('kid', 0.6823671121096673), ('chop', 0.6742604057487516)] \n",
      "\n",
      "PENN addition:\n",
      " [('girl', 0.8391505005872348), ('aisha', 0.7091762278700058), ('catherine', 0.7079567266385662), ('aphrodite', 0.7056407155880299), ('margaret', 0.6983328860558038), ('wicked', 0.6932060399283371), ('pregnant', 0.6914340390000767), ('gabriel', 0.6898858371339746), ('pretend', 0.6894373559795783), ('pitcher', 0.6885295168140809)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(we_word2vec, we_penn, \"boy\", \"girl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
