{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "class WordEmbeddings():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.word_frequency = dict()\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.embeddings = None\n",
    "        for i, line in enumerate(open(self.filename, encoding=\"utf8\")):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.vocab_size = int(line[0])\n",
    "                self.embedding_size = int(line[1])\n",
    "                self.embeddings = np.zeros((self.vocab_size, self.embedding_size))\n",
    "                continue\n",
    "            line = line.split(' ', 1)\n",
    "            word = line[0]\n",
    "            self.word2id[word] = i - 1\n",
    "            self.id2word[i - 1] = word\n",
    "            self.embeddings[i - 1, :] = np.fromstring(line[1], dtype=float, sep=' ')\n",
    "    \n",
    "    def embedding_for(self, word):\n",
    "        ind = self.word2id[word]\n",
    "        return self.embeddings[ind, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_penn = WordEmbeddings(\"./embeddings/out_enwik8_penn_500dim_5wind.vec\")\n",
    "we_word2vec = WordEmbeddings(\"./embeddings/out_enwik8_w2v.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def vectorize_word(we, word):\n",
    "    vec = None\n",
    "    if isinstance(word, np.ndarray):\n",
    "        vec = word\n",
    "    if isinstance(word, str):\n",
    "        vec = we.embedding_for(word)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    vec = vec.reshape((1, -1))\n",
    "    return cosine_similarity(vec, vec2)\n",
    "  \n",
    "    \n",
    "def nearest_words(we, word, top_n=10):\n",
    "    vec = vectorize_word(we, word)\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def nearest_words_to_pairs_addition(we, word_a, word_b, top_n=10):\n",
    "    vec_a = vectorize_word(we, word_a)\n",
    "    vec_b = vectorize_word(we, word_b)\n",
    "    vec = vec_a + vec_b\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def best_cosine(we, top_n=10):\n",
    "    best_cosins = np.zeros(we.vocab_size, dtype=\"float\")\n",
    "    best_cos_pair = np.zeros(we.vocab_size, dtype=\"int64\")\n",
    "    for ind in tqdm(we.word2id.values()):\n",
    "        #ind = 0\n",
    "        #if ind > 100:\n",
    "            #break\n",
    "        vec = we.embeddings[ind, :]\n",
    "        vec = vec.reshape((1, -1))\n",
    "        cosines = cosine_similarity(vec, we.embeddings)\n",
    "        #word_id = np.argsort(cosines[0])[::-1][1] \n",
    "        word_id = np.argpartition(cosines[0],-2)[-2:][0]\n",
    "        #print(np.argpartition(cosines[0],-2)[-2:])\n",
    "        best_cosins[ind] = cosines[0][word_id]\n",
    "        best_cos_pair[ind] = word_id\n",
    "    #print(best_cosins[:110])\n",
    "    #print(best_cos_pair[:110])\n",
    "    top_cos_args_id = np.argsort(best_cosins)[::-1][0:top_n]\n",
    "    best_pairs = [(we.id2word[ind], we.id2word[best_cos_pair[ind]], best_cosins[ind]) \n",
    "                  for i, ind in enumerate(top_cos_args_id)]\n",
    "    return best_pairs\n",
    "    \n",
    "    \n",
    "def compare(we_a, we_b, word_a, word_b):\n",
    "    print(\"Words: {} + {}\".format(word_a, word_b))\n",
    "    print(\"Word2vec addition:\\n\", nearest_words_to_pairs_addition(we_a, word_b, word_a), \"\\n\")\n",
    "    print(\"PENN addition:\\n\", nearest_words_to_pairs_addition(we_b, word_b, word_a), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a070a10a2441b5a3ac2a3e44fb1e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11731), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('glycolysis', 'spotter', 0.9876011089363196),\n",
       " ('spotter', 'glycolysis', 0.9876011089363196),\n",
       " ('archaeoastronomy', 'spotter', 0.9874648536048432),\n",
       " ('kapoor', 'archaeoastronomy', 0.987102067265258),\n",
       " ('breakdanc', 'spotter', 0.9869388159123954),\n",
       " ('heckel', 'spotter', 0.9869177410172493),\n",
       " ('anoa', 'glycolysis', 0.9865203028149112),\n",
       " ('breakdance', 'spotter', 0.9863924243607858),\n",
       " ('abhidharma', 'glycolysis', 0.9862122248412486),\n",
       " ('chime', 'spotter', 0.9861884005240806)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cosine(we_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf608ed2c4f4162a2e5fad3aac2d513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11731), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('shine', 'boas', 0.9972787421258333),\n",
       " ('boas', 'shine', 0.9972787421258333),\n",
       " ('uke', 'shine', 0.9972310205652469),\n",
       " ('statistician', 'shine', 0.9971986678012923),\n",
       " ('cin', 'narnia', 0.9971982634743759),\n",
       " ('narnia', 'cin', 0.9971982634743759),\n",
       " ('belisarius', 'akkad', 0.9971779044314791),\n",
       " ('akkad', 'belisarius', 0.9971779044314791),\n",
       " ('abram', 'alexandrine', 0.9971627087764258),\n",
       " ('alexandrine', 'abram', 0.9971627087764258)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cosine(we_penn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sixth', 0.6719917776923781),\n",
       " ('fifth', 0.6649193634769768),\n",
       " ('ninth', 0.6450301806544565),\n",
       " ('bah', 0.6432698042673961),\n",
       " ('coronation', 0.6260082211067375),\n",
       " ('ceremony', 0.6247439766376065),\n",
       " ('tenth', 0.600044981236633),\n",
       " ('commemorate', 0.5960178614840119),\n",
       " ('eighth', 0.5956747627772887),\n",
       " ('wait', 0.5913010259671003)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_word2vec, \"fourth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scot', 0.628178408225462),\n",
       " ('poland', 0.6197486665356933),\n",
       " ('milan', 0.6193810941023434),\n",
       " ('ratification', 0.6181803721903307),\n",
       " ('revolt', 0.6169001725420755),\n",
       " ('legion', 0.6150062996986057),\n",
       " ('abdicate', 0.6145488671710846),\n",
       " ('crusade', 0.6124782108609884),\n",
       " ('consul', 0.6094307030571164),\n",
       " ('quebec', 0.6082644753267177)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_penn, \"fourth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.90739676267495),\n",
       " ('thirteen', 0.729378889200718),\n",
       " ('teenage', 0.7238851521057705),\n",
       " ('beautiful', 0.7080131873160405),\n",
       " ('rap', 0.7046934852354403),\n",
       " ('astro', 0.6931712679654263),\n",
       " ('marple', 0.69063418681593),\n",
       " ('sibling', 0.685468327135792),\n",
       " ('kid', 0.6823671121096673),\n",
       " ('chop', 0.6742604057487516)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_word2vec, 'boy', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8391505005872348),\n",
       " ('aisha', 0.7091762278700058),\n",
       " ('catherine', 0.7079567266385662),\n",
       " ('aphrodite', 0.7056407155880299),\n",
       " ('margaret', 0.6983328860558038),\n",
       " ('wicked', 0.6932060399283371),\n",
       " ('pregnant', 0.6914340390000767),\n",
       " ('gabriel', 0.6898858371339746),\n",
       " ('pretend', 0.6894373559795783),\n",
       " ('pitcher', 0.6885295168140809)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_penn, 'boy', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: boy + girl\n",
      "Word2vec addition:\n",
      " [('boy', 0.90739676267495), ('thirteen', 0.729378889200718), ('teenage', 0.7238851521057705), ('beautiful', 0.7080131873160405), ('rap', 0.7046934852354403), ('astro', 0.6931712679654263), ('marple', 0.69063418681593), ('sibling', 0.685468327135792), ('kid', 0.6823671121096673), ('chop', 0.6742604057487516)] \n",
      "\n",
      "PENN addition:\n",
      " [('girl', 0.8391505005872348), ('aisha', 0.7091762278700058), ('catherine', 0.7079567266385662), ('aphrodite', 0.7056407155880299), ('margaret', 0.6983328860558038), ('wicked', 0.6932060399283371), ('pregnant', 0.6914340390000767), ('gabriel', 0.6898858371339746), ('pretend', 0.6894373559795783), ('pitcher', 0.6885295168140809)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(we_word2vec, we_penn, \"boy\", \"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_penn_ga = WordEmbeddings(\"./embeddings/out_penn_google_analogy_500dim_5w.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Damascus', 0.14716633719577282),\n",
       " ('Nicaragua', 0.13604063381083042),\n",
       " ('Malawi', 0.12357785844605604),\n",
       " ('Rome', 0.12042452563616046),\n",
       " ('Tirana', 0.10601653362005484),\n",
       " ('Denmark', 0.0984432185515497),\n",
       " ('girl', 0.09779731242189263),\n",
       " ('Colorado', 0.09714938057236729),\n",
       " ('Sweden', 0.09171789368034912),\n",
       " ('Alaska', 0.08876820343191807)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_penn_ga, \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.5801384539889545), ('Zagreb', 0.14332272094655746), ('Malawi', 0.12356903196667185), ('Damascus', 0.10882978565101585), ('Sweden', 0.10253285283701401), ('Paramaribo', 0.10219196281670165), ('Egypt', 0.09706241482279429), ('Rome', 0.09327041103238491), ('hryvnia', 0.0922410525632594), ('Astana', 0.0889859372757664)]\n"
     ]
    }
   ],
   "source": [
    "def fff(we):\n",
    "    vec_a = vectorize_word(we, \"king\")\n",
    "    vec_b = vectorize_word(we, \"queen\")\n",
    "    vec_c = vectorize_word(we, \"woman\")\n",
    "    vec = vec_a - vec_b + vec_c\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:10+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    print(neighbors)\n",
    "fff(we_penn_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_w2v_ga = WordEmbeddings(\"./embeddings/out_w2v_google_analogy_500dim_5w.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diem.diem import Diem\n",
    "diem = Diem(\"./embeddings/out_diem.vec\")\n",
    "diem.learn_from_text(\"./data/enwik8_shorter_cleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mineral', 0.999719220718427),\n",
       " ('cerebral', 0.9995886006419438),\n",
       " ('generous', 0.9995176969374526),\n",
       " ('liberal', 0.9994882096291209),\n",
       " ('renewal', 0.9994664576392052),\n",
       " ('generic', 0.9994605577604865),\n",
       " ('deliver', 0.9994532622650988),\n",
       " ('revival', 0.9994483966530653),\n",
       " ('beverly', 0.9994041409082012),\n",
       " ('rearden', 0.9993923162858849)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(diem, \"general\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foul', 0.9987633315785385),\n",
       " ('pour', 0.9985929568619805),\n",
       " ('paul', 0.9985834929228972),\n",
       " ('gil', 0.9985640825607383),\n",
       " ('kirk', 0.9985392757949823),\n",
       " ('lyon', 0.9985130765569822),\n",
       " ('baby', 0.9984835679902364),\n",
       " ('dual', 0.9984803096283539),\n",
       " ('loud', 0.9984690391061233),\n",
       " ('milk', 0.9984578617722678)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(diem, 'boy', 'girl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
