{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class WordEmbeddings():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.word_frequency = dict()\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.embeddings = None\n",
    "        for i, line in enumerate(open(self.filename, encoding=\"utf8\")):\n",
    "            if i == 0:\n",
    "                line = line.split()\n",
    "                self.vocab_size = int(line[0])\n",
    "                self.embedding_size = int(line[1])\n",
    "                self.embeddings = np.zeros((self.vocab_size, self.embedding_size))\n",
    "                continue\n",
    "            line = line.split(' ', 1)\n",
    "            word = line[0]\n",
    "            self.word2id[word] = i - 1\n",
    "            self.id2word[i - 1] = word\n",
    "            self.embeddings[i - 1, :] = np.fromstring(line[1], dtype=float, sep=' ')\n",
    "    \n",
    "    def embedding_for(self, word):\n",
    "        ind = self.word2id[word]\n",
    "        return self.embeddings[ind, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_penn = WordEmbeddings(\"./embeddings/out_enwik8_penn_500dim_5wind.vec\")\n",
    "we_word2vec = WordEmbeddings(\"./embeddings/out_enwik8_w2v.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-71-768981401c8f>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-768981401c8f>\"\u001b[1;36m, line \u001b[1;32m58\u001b[0m\n\u001b[1;33m    = [(we.id2word[word_ind], cosines[0][top10_ind[i]]) for i, word_ind in enumerate(top_cos_args_id)]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def vectorize_word(we, word):\n",
    "    vec = None\n",
    "    if isinstance(word, np.ndarray):\n",
    "        vec = word\n",
    "    if isinstance(word, str):\n",
    "        vec = we.embedding_for(word)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    vec = vec.reshape((1, -1))\n",
    "    return cosine_similarity(vec, vec2)\n",
    "  \n",
    "    \n",
    "def nearest_words(we, word, top_n=10):\n",
    "    vec = vectorize_word(we, word)\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][word_ind]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def nearest_words_to_pairs_addition(we, word_a, word_b, top_n=10):\n",
    "    vec_a = vectorize_word(we, word_a)\n",
    "    vec_b = vectorize_word(we, word_b)\n",
    "    vec = vec_a + vec_b\n",
    "    vec = vec.reshape((1, -1))\n",
    "    cosines = cosine_similarity(vec, we.embeddings)\n",
    "    top10_ind = np.argsort(cosines)[0][::-1][1:top_n+1]\n",
    "    neighbors = [(we.id2word[word_ind], cosines[0][top10_ind[i]]) for i, word_ind in enumerate(top10_ind)]\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def best_cosine(we, ton_n=10):\n",
    "    best_cosins = np.zeros(we.vocab_size, dtype=\"float\")\n",
    "    best_cos_pair = np.zeros(we.vocab_size, dtype=\"int64\")\n",
    "    for ind in we.word2id.values():\n",
    "        #ind = 0\n",
    "        vec = we.embeddings[ind, :]\n",
    "        vec = vec.reshape((1, -1))\n",
    "        cosines = cosine_similarity(vec, we.embeddings)\n",
    "        #print(cosines)\n",
    "        top = np.argsort(cosines)[0][::-1][1:top_n+1] \n",
    "        #print(top)\n",
    "        word_id = None\n",
    "        #if(top[0] == ind):\n",
    "            #word_id = top[1]\n",
    "        #else:\n",
    "        word_id = top[0]\n",
    "        #print(word_id)\n",
    "        #print(we.id2word[word_id])\n",
    "        #print(cos)\n",
    "        best_cosins[ind] = cosines[0][word_id]\n",
    "        best_cos_pair[ind] = word_id\n",
    "    top_cos_args_id = np.argmax(best_cosins)\n",
    "    best_pairs = [(we.id2word[ind], best_cos_pair[ind], best_cosins[ind]) \n",
    "                  for i, ind in enumerate(top_cos_args_id)]\n",
    "\n",
    "    top_best_cos_pair = best_cos_pair[top_cos_args_id]\n",
    "    \n",
    "    \n",
    "def compare(we_a, we_b, word_a, word_b):\n",
    "    print(\"Words: {} + {}\".format(word_a, word_b))\n",
    "    print(\"Word2vec addition:\\n\", nearest_words_to_pairs_addition(we_a, word_b, word_a), \"\\n\")\n",
    "    print(\"PENN addition:\\n\", nearest_words_to_pairs_addition(we_b, word_b, word_a), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.21555466  0.29981455 ... -0.01095824  0.03517678\n",
      "   0.01968718]]\n",
      "[   0  773  343 ... 7283 6648 6101]\n",
      "773\n",
      "capitalist\n",
      "0.9329881297461052\n"
     ]
    }
   ],
   "source": [
    "best_cosine(we_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('capitalist', 0.9329881297461052),\n",
       " ('capitalism', 0.9026125478341888),\n",
       " ('anarchist', 0.8945130504426946),\n",
       " ('anarcho', 0.8944931630226096),\n",
       " ('libertarian', 0.8798084819004081),\n",
       " ('faire', 0.867901212119998),\n",
       " ('liberalism', 0.8544044630409507),\n",
       " ('laissez', 0.8279008774309387),\n",
       " ('rothbard', 0.8255568321545554),\n",
       " ('communism', 0.8160838067515472)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_word2vec, \"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('individualist', 0.8593052069099795),\n",
       " ('anarchist', 0.8471530492275303),\n",
       " ('rothbard', 0.8316277232633484),\n",
       " ('metaphysical', 0.8207106134074114),\n",
       " ('zionism', 0.8126567946987308),\n",
       " ('rejection', 0.8126141317278897),\n",
       " ('authoritarian', 0.8109840579306388),\n",
       " ('contend', 0.8101501365925213),\n",
       " ('aclu', 0.8078392413248282),\n",
       " ('contradict', 0.8075189621300534)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words(we_penn, \"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soviet', 0.8919573507156756),\n",
       " ('guerrilla', 0.7668216671653967),\n",
       " ('warsaw', 0.7589129466196749),\n",
       " ('dissident', 0.7412740590799167),\n",
       " ('veteran', 0.7346475360194393),\n",
       " ('coup', 0.7303312014641579),\n",
       " ('liberate', 0.7273080260705842),\n",
       " ('neutrality', 0.7189481155538215),\n",
       " ('pact', 0.7172397340185512),\n",
       " ('ussr', 0.712835335414549)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_word2vec, 'soviet', 'union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('union', 0.8449629208665597),\n",
       " ('confederate', 0.6656608921533554),\n",
       " ('slave', 0.6369609214171088),\n",
       " ('invasion', 0.6344407750103969),\n",
       " ('warsaw', 0.6328482311938771),\n",
       " ('ally', 0.6253420991806693),\n",
       " ('secession', 0.6177441658627295),\n",
       " ('occupation', 0.6123611501317596),\n",
       " ('seize', 0.598720415174633),\n",
       " ('liberty', 0.5952388067064349)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words_to_pairs_addition(we_penn, 'soviet', 'union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: boy + girl\n",
      "Word2vec addition:\n",
      " [('boy', 0.90739676267495), ('thirteen', 0.729378889200718), ('teenage', 0.7238851521057705), ('beautiful', 0.7080131873160405), ('rap', 0.7046934852354403), ('astro', 0.6931712679654263), ('marple', 0.69063418681593), ('sibling', 0.685468327135792), ('kid', 0.6823671121096673), ('chop', 0.6742604057487516)] \n",
      "\n",
      "PENN addition:\n",
      " [('girl', 0.8391505005872348), ('aisha', 0.7091762278700058), ('catherine', 0.7079567266385662), ('aphrodite', 0.7056407155880299), ('margaret', 0.6983328860558038), ('wicked', 0.6932060399283371), ('pregnant', 0.6914340390000767), ('gabriel', 0.6898858371339746), ('pretend', 0.6894373559795783), ('pitcher', 0.6885295168140809)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(we_word2vec, we_penn, \"boy\", \"girl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
